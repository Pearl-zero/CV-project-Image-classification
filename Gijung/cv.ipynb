{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import optuna\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import cv2\n",
    "import json\n",
    "import gc \n",
    "from datetime import datetime\n",
    "\n",
    "# 시드 고정\n",
    "def set_seed(SEED=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "train_img_path = \"../data/train/\"\n",
    "train_csv_path = \"../data/train.csv\"\n",
    "\n",
    "test_img_path = \"../data/test/\"\n",
    "sample_path = \"../data/sample_submission.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, path, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: DataFrame 또는 CSV 파일 경로\n",
    "            path: 이미지 파일들이 있는 디렉토리 경로\n",
    "            transform: 이미지 변환 함수\n",
    "        \"\"\"\n",
    "        if isinstance(data, str):\n",
    "            self.df = pd.read_csv(data).values\n",
    "        else:\n",
    "            self.df = data.values\n",
    "            \n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        \n",
    "        # RGB가 아닌 이미지 처리\n",
    "        if len(img.shape) == 2:\n",
    "            img = np.stack([img] * 3, axis=-1)\n",
    "        elif img.shape[2] == 4:\n",
    "            img = img[:, :, :3]\n",
    "            \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target\n",
    "\n",
    "def get_transforms(img_size):\n",
    "    trn_transform = A.Compose([\n",
    "        A.OneOf([\n",
    "        A.GridDistortion(num_steps=5, distort_limit=0.3, interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=True, p=1),\n",
    "        A.ElasticTransform(always_apply=True, p=1, alpha=1.0, sigma=50.0, alpha_affine=50.0, interpolation=0, border_mode=1, value=(0, 0, 0), mask_value=None, approximate=False),\n",
    "        A.OpticalDistortion(always_apply=True, p=1, distort_limit=(-0.3, -0.1)),\n",
    "        A.OpticalDistortion(always_apply=True, p=1, distort_limit=(0.1, 0.3)),\n",
    "        ], p=0.85),\n",
    "        A.SomeOf([\n",
    "            A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=1),\n",
    "            A.HueSaturationValue(hue_shift_limit=15, sat_shift_limit=25, val_shift_limit=20, p=1),\n",
    "            A.MultiplicativeNoise(p=1, multiplier=(1, 1.5), per_channel=True),\n",
    "            A.Equalize(p=1, mode='cv', by_channels=True),\n",
    "        ], n=2, p=0.85),\n",
    "        A.OneOf([\n",
    "            A.Rotate(limit=(10, 30), border_mode=cv2.BORDER_CONSTANT, p=1),\n",
    "            A.Rotate(limit=(150, 170), border_mode=cv2.BORDER_CONSTANT, p=1),\n",
    "            A.Rotate(limit=(190, 210), border_mode=cv2.BORDER_CONSTANT, p=1),\n",
    "            A.Rotate(limit=(330, 350), border_mode=cv2.BORDER_CONSTANT, p=1),\n",
    "        ], p=1),\n",
    "        A.CoarseDropout(p=0.5, max_holes=40, max_height=15, max_width=15, min_holes=8, min_height=8, min_width=8),\n",
    "        A.Equalize(p=0.5, mode='cv', by_channels=True),\n",
    "        A.OneOf([\n",
    "            A.Blur(blur_limit=(3, 4), p=1),\n",
    "            A.MotionBlur(blur_limit=(3, 5), p=1),\n",
    "            A.Downscale(scale_min=0.455, scale_max=0.5, interpolation=2, p=1),\n",
    "        ], p=0.5),\n",
    "        A.GaussNoise(var_limit=(100, 800), per_channel=True, p=0.5),\n",
    "        A.RandomRotate90(),\n",
    "        A.CLAHE(p=0.5),\n",
    "        A.MotionBlur(p=0.2),\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    tst_transform = A.Compose([\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    return trn_transform, tst_transform\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_class_metrics(y_true, y_pred, num_classes=17):\n",
    "    \"\"\"각 클래스별 accuracy와 f1 score를 계산\"\"\"\n",
    "    class_metrics = {}\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        # 해당 클래스에 대한 이진 레이블 생성\n",
    "        y_true_binary = (y_true == class_idx)\n",
    "        y_pred_binary = (y_pred == class_idx)\n",
    "        \n",
    "        # 클래스별 metrics 계산\n",
    "        class_acc = accuracy_score(y_true_binary, y_pred_binary)\n",
    "        class_f1 = f1_score(y_true_binary, y_pred_binary, average='binary')\n",
    "        support = np.sum(y_true_binary)\n",
    "        \n",
    "        class_metrics[f'class_{class_idx}'] = {\n",
    "            'accuracy': class_acc,\n",
    "            'f1_score': class_f1,\n",
    "            'support': support\n",
    "        }\n",
    "        \n",
    "    # 전체 metrics\n",
    "    overall_acc = accuracy_score(y_true, y_pred)\n",
    "    overall_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    class_metrics['overall'] = {\n",
    "        'accuracy': overall_acc,\n",
    "        'f1_score': overall_f1\n",
    "    }\n",
    "    \n",
    "    return class_metrics    \n",
    "        \n",
    "        \n",
    "        \n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader, leave=False)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    return {\n",
    "        \"loss\": train_loss,\n",
    "        \"acc\": train_acc,\n",
    "        \"f1\": train_f1,\n",
    "    }\n",
    "\n",
    "def valid_one_epoch(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "    valid_loss /= len(loader)\n",
    "    metrics = calculate_class_metrics(np.array(targets_list), np.array(preds_list))\n",
    "\n",
    "    return {\n",
    "        \"loss\": valid_loss,\n",
    "        \"metrics\": metrics\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_best_params(params, device):\n",
    "    \"\"\"베스트 파라미터로 k-fold 검증 수행\"\"\"\n",
    "    print(\"\\nEvaluating best parameters with k-fold validation...\")\n",
    "    \n",
    "    if params['model_name'] == \"swinv2_tiny_window8_256\":\n",
    "        img_size = 256\n",
    "    elif params['model_name'] == \"tf_efficientnet_b3.ns_jft_in1k\":\n",
    "        img_size = 300\n",
    "        \n",
    "    trn_transform, _ = get_transforms(img_size)\n",
    "    \n",
    "    # -----------------------------------------------------------------------------\n",
    "    # K-fold 검증 준비\n",
    "    n_splits = 5\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    # 전체 데이터셋 로드\n",
    "    full_dataset = ImageDataset(train_csv_path, train_img_path, transform=trn_transform)\n",
    "    \n",
    "    # 데이터와 라벨 분리\n",
    "    data = pd.read_csv(train_csv_path)\n",
    "    X = np.arange(len(data))\n",
    "    y = data['target'].values\n",
    "    \n",
    "    # 각 fold의 결과를 저장할 리스트\n",
    "    fold_results = []\n",
    "    \n",
    "    # K-fold 교차 검증\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "        print(f\"\\n{'='*20} Fold {fold} {'='*20}\")\n",
    "        \n",
    "        # 데이터셋 분할\n",
    "        train_dataset = Subset(full_dataset, train_idx)\n",
    "        val_dataset = Subset(full_dataset, val_idx)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=params['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=params['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # 모델 설정\n",
    "        model = timm.create_model(\n",
    "            params['model_name'],\n",
    "            pretrained=True,\n",
    "            num_classes=17\n",
    "        ).to(device)\n",
    "        \n",
    "        optimizer_class = getattr(torch.optim, params['optimizer'])\n",
    "        optimizer = optimizer_class(model.parameters(), \n",
    "                                  lr=params['lr'], \n",
    "                                  weight_decay=params['weight_decay'])\n",
    "        \n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # -----------------------------------------------------------------------------\n",
    "        best_val_f1 = 0\n",
    "        best_epoch_metrics = None\n",
    "        patience = 5\n",
    "        patience_counter = 0\n",
    "        EPOCH = 30\n",
    "        \n",
    "        # 학습\n",
    "        for epoch in range(EPOCH):\n",
    "            train_ret = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "            val_ret = valid_one_epoch(val_loader, model, loss_fn, device)\n",
    "            \n",
    "            current_val_f1 = val_ret['metrics']['overall']['f1_score']\n",
    "            \n",
    "            if current_val_f1 > best_val_f1:\n",
    "                best_val_f1 = current_val_f1\n",
    "                best_epoch_metrics = val_ret['metrics']\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if not epoch % 5:\n",
    "                print(f\"Epoch {epoch+1}: Val F1 = {current_val_f1:.4f}\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "            \n",
    "\n",
    "                    \n",
    "        \n",
    "        # 현재 fold의 best 결과 출력\n",
    "        print(f\"\\nBest Results for Fold {fold}:\")\n",
    "        print(\"\\nClass-wise Metrics:\")\n",
    "        for class_idx in range(17):\n",
    "            metrics = best_epoch_metrics[f'class_{class_idx}']\n",
    "            print(f\"\\nClass {class_idx}:\")\n",
    "            print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "            print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
    "            print(f\"Support: {metrics['support']}\")\n",
    "        \n",
    "        # fold 결과 저장\n",
    "        fold_results.append({\n",
    "            'fold': fold,\n",
    "            'metrics': best_epoch_metrics\n",
    "        })\n",
    "    \n",
    "    # 모든 fold의 평균 성능 계산 및 출력\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Average Performance Across All Folds:\")\n",
    "    \n",
    "    # 클래스별 평균 성능\n",
    "    class_avg_metrics = {}\n",
    "    for class_idx in range(17):\n",
    "        accuracies = [fold['metrics'][f'class_{class_idx}']['accuracy'] for fold in fold_results]\n",
    "        f1_scores = [fold['metrics'][f'class_{class_idx}']['f1_score'] for fold in fold_results]\n",
    "        \n",
    "        avg_acc = np.mean(accuracies)\n",
    "        avg_f1 = np.mean(f1_scores)\n",
    "        std_acc = np.std(accuracies)\n",
    "        std_f1 = np.std(f1_scores)\n",
    "        \n",
    "        print(f\"\\nClass {class_idx}:\")\n",
    "        print(f\"Accuracy: {avg_acc:.4f} (±{std_acc:.4f})\")\n",
    "        print(f\"F1 Score: {avg_f1:.4f} (±{std_f1:.4f})\")\n",
    "    \n",
    "    # 결과를 DataFrame으로 저장\n",
    "    results_dict = {\n",
    "        'fold': [],\n",
    "        'class': [],\n",
    "        'accuracy': [],\n",
    "        'f1_score': [],\n",
    "        'support': []\n",
    "    }\n",
    "    \n",
    "    for fold_result in fold_results:\n",
    "        fold_num = fold_result['fold']\n",
    "        metrics = fold_result['metrics']\n",
    "        \n",
    "        for class_idx in range(17):\n",
    "            class_metrics = metrics[f'class_{class_idx}']\n",
    "            results_dict['fold'].append(fold_num)\n",
    "            results_dict['class'].append(class_idx)\n",
    "            results_dict['accuracy'].append(class_metrics['accuracy'])\n",
    "            results_dict['f1_score'].append(class_metrics['f1_score'])\n",
    "            results_dict['support'].append(class_metrics['support'])\n",
    "    \n",
    "    results_df = pd.DataFrame(results_dict)\n",
    "    now = datetime.now()\n",
    "    results_df.to_csv(f'best_params_fold_class_metrics_{now.month:02}{now.day:02}{now.hour:02}{now.minute:02}.csv', index=False)\n",
    "    print(\"\\nDetailed metrics saved to 'best_params_fold_class_metrics.csv'\")\n",
    "    \n",
    "    return fold_results\n",
    "\n",
    "\n",
    "\n",
    "def generate_pseudo_labels(model, loader, device):\n",
    "    model.eval()\n",
    "    pseudo_labels = []\n",
    "    confidences = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image, _ in tqdm(loader, desc=\"Generating pseudo labels\"):\n",
    "            image = image.to(device)\n",
    "            outputs = model(image)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            conf, preds = torch.max(probs, dim=1)\n",
    "            pseudo_labels.extend(preds.cpu().numpy())\n",
    "            confidences.extend(conf.cpu().numpy())\n",
    "            \n",
    "    return np.array(pseudo_labels), np.array(confidences)\n",
    "\n",
    "def objective(trial , device):\n",
    "    try:\n",
    "        \n",
    "        print(f\"GPU Memory before trial: {torch.cuda.memory_allocated(device)/(1024**2):.2f}MB\")\n",
    "        # 하이퍼파라미터 탐색 공간 정의\n",
    "        params = {\n",
    "            # 'model_name': trial.suggest_categorical('model_name', ['swinv2_tiny_window8_256', 'efficientnet_b0', 'resnet18']),\n",
    "            'model_name': trial.suggest_categorical('model_name', ['tf_efficientnet_b3.ns_jft_in1k']),\n",
    "            # 'img_size': trial.suggest_categorical('img_size', [256]),\n",
    "            'batch_size': trial.suggest_categorical('batch_size', [8,16,32]),\n",
    "            'lr': trial.suggest_float('lr', 1e-5, 1e-3, log=True),\n",
    "            'weight_decay' : trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True),\n",
    "            'dropout_rate' : trial.suggest_float('dropout_rate', 0.0, 0.3),\n",
    "            'optimizer': trial.suggest_categorical('optimizer', ['Adam', 'AdamW']),\n",
    "        }\n",
    "    \n",
    "        \n",
    "        # 모델 선택 및 입력 이미지 크기 설정\n",
    "        if params['model_name'] == \"swinv2_tiny_window8_256\":\n",
    "            params['img_size'] = 256\n",
    "        elif params['model_name'] == \"tf_efficientnet_b3.ns_jft_in1k\":\n",
    "            params['img_size'] = 300\n",
    "            \n",
    "        # 데이터셋 준비\n",
    "        trn_transform, val_transform = get_transforms(params['img_size'])\n",
    "        \n",
    "        # -----------------------------------------------------------------------------\n",
    "        # K-fold 검증 준비\n",
    "        n_splits = 5\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        \n",
    "        # 전체 데이터셋 로드\n",
    "        full_dataset = ImageDataset(train_csv_path, train_img_path, transform=trn_transform)\n",
    "        \n",
    "        # 데이터와 라벨 분리\n",
    "        data = pd.read_csv(train_csv_path)\n",
    "        X = np.arange(len(data))\n",
    "        y = data['target'].values\n",
    "        \n",
    "        fold_scores = []\n",
    "        \n",
    "        # K-fold 교차 검증\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "            print(f\"\\nFold {fold}\")\n",
    "            \n",
    "            # 데이터셋 분할\n",
    "            train_dataset = Subset(full_dataset, train_idx)\n",
    "            val_dataset = Subset(full_dataset, val_idx)\n",
    "            \n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=params['batch_size'],\n",
    "                shuffle=True,\n",
    "                num_workers=4,\n",
    "                pin_memory=True\n",
    "            )\n",
    "            \n",
    "            val_loader = DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=params['batch_size'],\n",
    "                shuffle=False,\n",
    "                num_workers=4,\n",
    "                pin_memory=True\n",
    "            )\n",
    "            \n",
    "            # 모델 설정\n",
    "            model = timm.create_model(\n",
    "                params['model_name'],\n",
    "                pretrained=True,\n",
    "                num_classes=17\n",
    "            ).to(device)\n",
    "            \n",
    "            # Optimizer 설정\n",
    "            optimizer_class = getattr(torch.optim, params['optimizer'])\n",
    "            optimizer = optimizer_class(model.parameters(), lr=params['lr'])\n",
    "            \n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            \n",
    "            # -----------------------------------------------------------------------------\n",
    "            best_val_f1 = 0\n",
    "            patience = 5\n",
    "            patience_counter = 0\n",
    "            EPOCH = 30\n",
    "            \n",
    "            # 학습\n",
    "            for epoch in range(EPOCH):\n",
    "                train_ret = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "                val_ret = valid_one_epoch(val_loader, model, loss_fn, device)\n",
    "                current_val_f1 = val_ret['metrics']['overall']['f1_score']\n",
    "                \n",
    "                    \n",
    "                if not epoch % 10:\n",
    "                    print(f\"Epoch {epoch+1}: Train F1 = {train_ret['f1']:.4f}, Val F1 = {current_val_f1:.4f}\")\n",
    "                \n",
    "                if current_val_f1 > best_val_f1:\n",
    "                    best_val_f1 = current_val_f1\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping!\")\n",
    "                    break\n",
    "                \n",
    "                if current_val_f1 < 0.5:\n",
    "                    print(\"Early stopping!\")\n",
    "                    break\n",
    "         \n",
    "            fold_scores.append(best_val_f1)\n",
    "            \n",
    "        print(f\"GPU Memory after trial: {torch.cuda.memory_allocated(device)/(1024**2):.2f}MB\")    \n",
    "        \n",
    "        mean_f1 = np.mean(fold_scores)\n",
    "        return mean_f1\n",
    "    \n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "def main():\n",
    "    set_seed()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if device==\"cuda\": torch.cuda.empty_cache()\n",
    "    \n",
    "    # Optuna를 사용한 하이퍼파라미터 최적화\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        storage=optuna.storages.JournalStorage(\n",
    "            optuna.storages.JournalFileStorage(\"study.log\")  # 로그 파일로 저장\n",
    "        ),\n",
    "        load_if_exists=True\n",
    "    )\n",
    "    study.optimize(lambda trial: objective(trial, device), n_trials=50)\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    print(\"Best parameters:\", best_params)\n",
    "    \n",
    "    \n",
    "    # Best parameters 저장\n",
    "    now = datetime.now()\n",
    "    with open(f\"best_params_{now.month:02}{now.day:02}{now.hour:02}{now.minute:02}.json\", \"w\") as f:\n",
    "        json.dump(best_params, f)\n",
    "    print(\"Best parameters saved to best_params.json\")\n",
    "    \n",
    "    \n",
    "    # Best parameters 불러오기\n",
    "    # with open(\"best_params.json\", \"r\") as f:\n",
    "    #     loaded_params = json.load(f)\n",
    "    # print(\"Loaded best parameters:\", loaded_params)\n",
    "    \n",
    "    # 베스트 파라미터로 k-fold 검증 수행\n",
    "    fold_results = evaluate_best_params(best_params, device)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 최적의 하이퍼파라미터로 최종 모델 학습\n",
    "\n",
    "    if best_params['model_name'] == \"swinv2_tiny_window8_256\":\n",
    "        img_size = 256\n",
    "    elif best_params['model_name'] == \"tf_efficientnet_b3.ns_jft_in1k\":\n",
    "        img_size = 300\n",
    "            \n",
    "    # img_size = best_params['img_size']\n",
    "    trn_transform, tst_transform = get_transforms(img_size)\n",
    "    \n",
    "    # 전체 학습 데이터셋으로 모델 학습\n",
    "    trn_dataset = ImageDataset(train_csv_path, train_img_path, transform=trn_transform)\n",
    "    tst_dataset = ImageDataset(sample_path, test_img_path, transform=tst_transform)\n",
    "    \n",
    "    trn_loader = DataLoader(\n",
    "        trn_dataset,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    tst_loader = DataLoader(\n",
    "        tst_dataset,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 최종 모델 학습\n",
    "    model = timm.create_model(\n",
    "        best_params['model_name'],\n",
    "        pretrained=True,\n",
    "        num_classes=17\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer_class = getattr(torch.optim, best_params['optimizer'])\n",
    "    optimizer = optimizer_class(model.parameters(), lr=best_params['lr'])\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 초기 학습\n",
    "    print(\"Training initial model...\")\n",
    "    for epoch in range(30):\n",
    "        ret = train_one_epoch(trn_loader, model, optimizer, loss_fn, device)\n",
    "        print(f\"Epoch {epoch+1}: F1 = {ret['f1']:.4f}\")\n",
    "    \n",
    "    # Pseudo Labeling\n",
    "    print(\"\\nGenerating pseudo labels...\")\n",
    "    pseudo_labels, confidences = generate_pseudo_labels(model, tst_loader, device)\n",
    "    \n",
    "    # 높은 신뢰도의 예측만 선택 (임계값: 0.9)\n",
    "    confidence_threshold = 0.9\n",
    "    high_confidence_mask = confidences > confidence_threshold\n",
    "    \n",
    "    # Pseudo label 데이터 생성\n",
    "    test_data = pd.read_csv(sample_path)\n",
    "    pseudo_df = pd.DataFrame({\n",
    "        'ID': test_data['ID'][high_confidence_mask],\n",
    "        'target': pseudo_labels[high_confidence_mask]\n",
    "    })\n",
    "    \n",
    "    # Pseudo label 데이터로 추가 학습\n",
    "    print(f\"\\nFound {len(pseudo_df)} high-confidence pseudo labels\")\n",
    "    if len(pseudo_df) > 0:\n",
    "        pseudo_dataset = ImageDataset(pseudo_df, test_img_path, transform=trn_transform)\n",
    "        pseudo_loader = DataLoader(\n",
    "            pseudo_dataset,\n",
    "            batch_size=best_params['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # -----------------------------------------------------------------------------\n",
    "        print(\"Training with pseudo labels...\")\n",
    "        for epoch in range(20):\n",
    "            ret = train_one_epoch(pseudo_loader, model, optimizer, loss_fn, device)\n",
    "            print(f\"Pseudo Label Epoch {epoch+1}: F1 = {ret['f1']:.4f}\")\n",
    "    \n",
    "    \n",
    "    # 최종 예측 및 저장\n",
    "    print(\"\\nGenerating final predictions...\")\n",
    "    final_preds = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image, _ in tqdm(tst_loader):\n",
    "            image = image.to(device)\n",
    "            preds = model(image)\n",
    "            final_preds.extend(preds.argmax(dim=1).cpu().numpy())\n",
    "    \n",
    "    pred_df = pd.DataFrame({\n",
    "        'ID': test_data['ID'],\n",
    "        'target': final_preds\n",
    "    })\n",
    "    \n",
    "    pred_df.to_csv(f\"predictions_{now.month:02}{now.day:02}{now.hour:02}{now.minute:02}.csv\", index=False)\n",
    "    print(\"\\nPredictions saved to 'predictions.csv'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
