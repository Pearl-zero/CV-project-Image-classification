{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3BaoIkv5Xwa0",
    "outputId": "a3cb919d-9d6c-43d0-94b5-7ab412d2baf4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn, optim\n",
    "from augraphy import AugraphyPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9CQIA71-nE4r"
   },
   "outputs": [],
   "source": [
    "# 시드를 고정합니다.\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Hyl8oAy6TZAu"
   },
   "outputs": [],
   "source": [
    "# 데이터셋 클래스를 정의합니다.\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "moR5FfswnE4t"
   },
   "outputs": [],
   "source": [
    "class MixUp:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def mixup_data(self, x, y):\n",
    "        if self.alpha > 0:\n",
    "            lam = np.random.beta(self.alpha, self.alpha)\n",
    "        else:\n",
    "            lam = 1\n",
    "\n",
    "        batch_size = x.size()[0]\n",
    "        index = torch.randperm(batch_size)\n",
    "        mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "        y_a, y_b = y, y[index]\n",
    "        return mixed_x, y_a, y_b, lam\n",
    "\n",
    "    def mixup_loss(self, loss_fn, pred, labels_a, labels_b, lam):\n",
    "        return lam * loss_fn(pred, labels_a) + (1 - lam) * loss_fn(pred, labels_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NTI29xJdrPN3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "def cutmix_data(x, y, alpha=1.0):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size)\n",
    "\n",
    "    # Randomly select the bounding box for CutMix\n",
    "    width, height = x.size()[2], x.size()[3]\n",
    "    cut_ratio = np.sqrt(1. - lam)\n",
    "    cut_width = int(width * cut_ratio)\n",
    "    cut_height = int(height * cut_ratio)\n",
    "\n",
    "    # Randomly select the position of the bounding box\n",
    "    cx = np.random.randint(width)\n",
    "    cy = np.random.randint(height)\n",
    "\n",
    "    # Ensure bounding box coordinates are within image dimensions\n",
    "    bb_x1 = np.clip(cx - cut_width // 2, 0, width)\n",
    "    bb_x2 = np.clip(cx + cut_width // 2, 0, width)\n",
    "    bb_y1 = np.clip(cy - cut_height // 2, 0, height)\n",
    "    bb_y2 = np.clip(cy + cut_height // 2, 0, height)\n",
    "\n",
    "    # Create the CutMix images\n",
    "    mixed_x = x.clone()\n",
    "    mixed_x[:, :, bb_y1:bb_y2, bb_x1:bb_x2] = x[index, :, bb_y1:bb_y2, bb_x1:bb_x2]\n",
    "\n",
    "    # Return mixed images and corresponding labels\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def cutmix_loss(loss_fn, pred, labels_a, labels_b, lam):\n",
    "    return lam * loss_fn(pred, labels_a) + (1 - lam) * loss_fn(pred, labels_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bOk83a_msw_T"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device, epoch, mixup=None, cutmix=False):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Ensure targets is a tensor\n",
    "        if isinstance(targets, int):\n",
    "            targets = torch.tensor(targets)\n",
    "    \n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Determine if MixUp or CutMix should be applied\n",
    "        if epoch == 2 and mixup is not None:  # Apply MixUp in the third epoch (index 2)\n",
    "            mixed_image, targets_a, targets_b, lam = mixup.mixup_data(image, targets)\n",
    "            preds = model(mixed_image)\n",
    "            loss = mixup.mixup_loss(loss_fn, preds, targets_a, targets_b, lam)\n",
    "        \n",
    "        elif cutmix:  # Apply CutMix if it's enabled\n",
    "            mixed_image, targets_a, targets_b, lam = cutmix_data(image, targets)\n",
    "            preds = model(mixed_image)\n",
    "            loss = cutmix_loss(loss_fn, preds, targets_a, targets_b, lam)\n",
    "        \n",
    "        else:  # Regular training for other epochs\n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "        # Backpropagation and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update metrics\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Calculate average loss and metrics\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_f1\": train_f1,\n",
    "    }\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TYhwVLBnE4v"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/root/CV_PJT/CV_PJT/code/train4.csv')\n",
    "sample_submission_df = pd.read_csv(\"/root/CV_PJT/CV_PJT/data/data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wjom43UvoXcx"
   },
   "source": [
    "## 3. Hyper-parameters\n",
    "* 학습 및 추론에 필요한 하이퍼파라미터들을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KByfAeRmXwYk"
   },
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# data config\n",
    "# data_path = 'datasets_fin/'\n",
    "train_img_path = '/root/CV_PJT/CV_PJT/data/data/train'\n",
    "test_img_path = '/root/CV_PJT/CV_PJT/data/data/test'\n",
    "train_path = '/root/CV_PJT/CV_PJT/code/train2_3_7(14).csv'\n",
    "sub_path = '/root/CV_PJT/CV_PJT/data/data/sample_submission.csv'\n",
    "\n",
    "# model config\n",
    "model_name = 'efficientnet_b3'\n",
    "\n",
    "# training config\n",
    "img_size = 224\n",
    "LR = 1e-4\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "num_workers = 0\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            # true_dist = pred.data.clone()\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amum-FlIojc6"
   },
   "source": [
    "## 4. Load Data\n",
    "* 학습, 테스트 데이터셋과 로더를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "PqvA9AGdyFk6"
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from augraphy import *\n",
    "from augraphy.base.augmentationsequence import AugmentationSequence\n",
    "from augraphy.augmentations import *\n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from augraphy import AugmentationSequence, InkBleed, ColorPaper, Markup, Scribbles, BleedThrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "47s3-b4Nk_nu"
   },
   "outputs": [],
   "source": [
    "# Define AugraphyTransform as before\n",
    "class AugraphyTransform(ImageOnlyTransform):\n",
    "    def __init__(self, p=0.7, always_apply=False):\n",
    "        super(AugraphyTransform, self).__init__(p=p, always_apply=always_apply)\n",
    "        # Define the Augraphy pipeline with desired augmentations\n",
    "        self.pipeline = AugmentationSequence([\n",
    "            # Ink bleed effect\n",
    "            InkBleed(intensity_range=(0.1, 0.2), kernel_size=(3, 5), severity=(0.4, 0.6), p=0.5),\n",
    "            # Paper color changes\n",
    "            ColorPaper(hue_range=(-10, 10), saturation_range=(-30, 30), p=0.5),\n",
    "            # Markup\n",
    "            Markup(num_lines_range=(2, 5), p=0.5),\n",
    "            # Scribbles\n",
    "            Scribbles(scribbles_type=\"random\", scribbles_ink=\"random\", scribbles_location=\"random\",\n",
    "                      scribbles_size_range=(250, 600), scribbles_count_range=(1, 6), p=0.5),\n",
    "            # BleedThrough effect\n",
    "            BleedThrough(intensity_range=(0.1, 0.2), offsets=(10, 20), p=0.5),\n",
    "        ])\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        if not isinstance(img, np.ndarray):\n",
    "            img = np.array(img)\n",
    "        augmented_image = self.pipeline(img)[0]\n",
    "        return augmented_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Mpsxw5__lKwf"
   },
   "outputs": [],
   "source": [
    "# Define transformations with and without AugraphyTransform\n",
    "def get_transform(with_augraphy=True):\n",
    "    transforms = [\n",
    "        A.Resize(height=img_size, width=img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    "    if with_augraphy:\n",
    "        transforms.insert(4, AugraphyTransform(p=0.7))  # Insert AugraphyTransform before normalization\n",
    "    return A.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "llh5C7ZKoq2S"
   },
   "outputs": [],
   "source": [
    "# Initial transformations with AugraphyTransform\n",
    "trn_transform = get_transform(with_augraphy=True)\n",
    "tst_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ktwA-kynE4x",
    "outputId": "16ea1375-735f-4582-e2c3-cac89fdfd659"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 14080\n",
      "Validation dataset size: 3520\n",
      "Test dataset size: 3140\n"
     ]
    }
   ],
   "source": [
    "# Dataset definition\n",
    "full_dataset = ImageDataset(\n",
    "    train_path,\n",
    "    train_img_path,\n",
    "    transform=trn_transform\n",
    ")\n",
    "\n",
    "# Calculate the total number of samples in the dataset\n",
    "dataset_size = len(full_dataset)\n",
    "\n",
    "# Define the ratios for training and validation\n",
    "train_ratio = 0.8  # Use 80% of the data for training\n",
    "val_ratio = 1 - train_ratio  # Remaining 20% for validation\n",
    "\n",
    "# Calculate the number of samples for training and validation\n",
    "train_size = int(train_ratio * dataset_size)\n",
    "val_size = dataset_size - train_size  # Ensure all samples are accounted for\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "trn_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Define the test dataset\n",
    "tst_dataset = ImageDataset(\n",
    "    sub_path,\n",
    "    test_img_path,\n",
    "    transform=tst_transform\n",
    ")\n",
    "\n",
    "# Print the sizes of the datasets\n",
    "print(\"Training dataset size:\", len(trn_dataset))\n",
    "print(\"Validation dataset size:\", len(val_dataset))\n",
    "print(\"Test dataset size:\", len(tst_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_sO03fWaQj1h"
   },
   "outputs": [],
   "source": [
    "# DataLoader 정의\n",
    "trn_loader = DataLoader(\n",
    "    trn_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "tst_loader = DataLoader(\n",
    "    tst_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nmm5h3J-pXNV"
   },
   "source": [
    "## 5. Train Model\n",
    "* 모델을 로드하고, 학습을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "12ed2251de294e75b1b58ca4eff7895a",
      "287810d9b1fa4765a3833cada619a6c4",
      "ec9e1134340045b3bc301f540fb6e076",
      "fc8365dc38004f10b2da20f982550531",
      "a53f80c0db9e4bd89c210e80cf147fb0",
      "c29ede4e42784e2cb6ec9f2897b84d00",
      "309aad3b1b2e42538efd3c43ef2ee7ce",
      "3f685cc920cf4d8f8cdcfea9d7d006ed",
      "892cf8d9abfc439f89a334e5e742518f",
      "68ead17e3dba4e4483721871e20e59ff",
      "1ea4b022f29840078e224d33e4a60448"
     ]
    },
    "id": "FbBgFPsLT-CO",
    "outputId": "01b4b082-0c13-4830-bdbb-d442fa40b93c"
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model = timm.create_model(\n",
    "    model_name,\n",
    "    pretrained=True,\n",
    "    num_classes=17\n",
    ").to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=LR)\n",
    "\n",
    "criterion = LabelSmoothingLoss(classes=17, smoothing=0.1)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)  # e.g., reduce LR by a factor every 5 epochs\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "scheduler.step(0)  # Apply the initial learning rate adjustment if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/440 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<3>; VDepth = cv::impl::{anonymous}::Set<0, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 224\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m     ret \u001b[38;5;241m=\u001b[39m train_one_epoch(current_subset, model, optimizer, loss_fn, device, epoch, cutmix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrn_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# 결과 로깅\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(loader, model, optimizer, loss_fn, device, epoch, mixup, cutmix)\u001b[0m\n\u001b[1;32m      8\u001b[0m targets_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(loader)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, targets \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m     12\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     13\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, name)))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 15\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img, target\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/albumentations/core/composition.py:210\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    207\u001b[0m     p\u001b[38;5;241m.\u001b[39mpreprocess(data)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(transforms):\n\u001b[0;32m--> 210\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_each_transform:\n\u001b[1;32m    213\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_data_post_transform(data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/albumentations/core/transforms_interface.py:118\u001b[0m, in \u001b[0;36mBasicTransform.__call__\u001b[0;34m(self, force_apply, *args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m             warn(\n\u001b[1;32m    114\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_class_fullname() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m could work incorrectly in ReplayMode for other input data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m because its\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m params depend on targets.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m             )\n\u001b[1;32m    117\u001b[0m         kwargs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_key][\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)] \u001b[38;5;241m=\u001b[39m deepcopy(params)\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_with_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m kwargs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/albumentations/core/transforms_interface.py:131\u001b[0m, in \u001b[0;36mBasicTransform.apply_with_params\u001b[0;34m(self, params, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m     target_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_target_function(key)\n\u001b[1;32m    130\u001b[0m     target_dependencies \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_dependence\u001b[38;5;241m.\u001b[39mget(key, [])}\n\u001b[0;32m--> 131\u001b[0m     res[key] \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtarget_dependencies\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     res[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 23\u001b[0m, in \u001b[0;36mAugraphyTransform.apply\u001b[0;34m(self, img, **params)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     22\u001b[0m     img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img)\n\u001b[0;32m---> 23\u001b[0m augmented_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m augmented_image\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/augraphy/base/augmentationsequence.py:38\u001b[0m, in \u001b[0;36mAugmentationSequence.__call__\u001b[0;34m(self, image, layer, mask, keypoints, bounding_boxes, force)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m     37\u001b[0m     result \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 38\u001b[0m current_result \u001b[38;5;241m=\u001b[39m \u001b[43maugmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeypoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeypoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounding_boxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounding_boxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(augmentation, Augmentation):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (keypoints \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (bounding_boxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/augraphy/augmentations/colorpaper.py:78\u001b[0m, in \u001b[0;36mColorPaper.__call__\u001b[0;34m(self, image, layer, mask, keypoints, bounding_boxes, force)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_run():\n\u001b[1;32m     76\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 78\u001b[0m     color_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_color\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# check for additional output of mask, keypoints and bounding boxes\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     outputs_extra \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/augraphy/augmentations/colorpaper.py:53\u001b[0m, in \u001b[0;36mColorPaper.add_color\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     50\u001b[0m ysize, xsize \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# convert to hsv colorspace\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m image_hsv \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2HSV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# assign hue and saturation\u001b[39;00m\n\u001b[1;32m     55\u001b[0m image_h \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhue_range[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhue_range[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m(ysize, xsize))\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<3>; VDepth = cv::impl::{anonymous}::Set<0, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 224\n"
     ]
    }
   ],
   "source": [
    "# # Continue your training loop from start_epoch to EPOCHS\n",
    "# for epoch in range(EPOCHS):\n",
    "#     if epoch == 0:\n",
    "#         trn_dataset.transform = get_transform(with_augraphy=True)\n",
    "        \n",
    "#     # 데이터셋 epoch 업데이트\n",
    "#         trn_dataset.epoch = epoch\n",
    "        \n",
    "#         # 세 번째 에포크부터는 데이터의 30%만 사용\n",
    "#     if epoch >= 3:\n",
    "#         indices = list(range(len(trn_dataset)))\n",
    "#         random.shuffle(indices)\n",
    "#         subset_size = int(0.5 * len(trn_dataset))\n",
    "#         subset_indices = indices[:subset_size]\n",
    "#         current_dataset = Subset(trn_dataset, subset_indices)\n",
    "#     else:\n",
    "#         current_dataset = trn_dataset\n",
    "            \n",
    "#     trn_loader = DataLoader(\n",
    "#         current_dataset, \n",
    "#         batch_size=BATCH_SIZE, \n",
    "#         shuffle=True,\n",
    "#         num_workers=0,\n",
    "#         pin_memory=True\n",
    "#     )\n",
    "\n",
    "#     # Determine if MixUp or CutMix should be applied\n",
    "#     if epoch == 1:\n",
    "#         # Create a subset of 30% for MixUp\n",
    "#         indices = list(range(len(current_dataset)))\n",
    "#         random.shuffle(indices)\n",
    "#         subset_size = int(0.3 * len(current_dataset))  # 30% of current dataset\n",
    "#         subset_indices = indices[:subset_size]\n",
    "#         current_subset = Subset(current_dataset, subset_indices)\n",
    "#         ret = train_one_epoch(current_subset, model, optimizer, loss_fn, device, epoch, mixup=MixUp)\n",
    "                              \n",
    "#     elif epoch == 2:\n",
    "#         # Create a subset of 30% for CutMix\n",
    "#         indices = list(range(len(current_dataset)))\n",
    "#         random.shuffle(indices)\n",
    "#         subset_size = int(0.3 * len(current_dataset))  # 30% of current dataset\n",
    "#         subset_indices = indices[:subset_size]\n",
    "#         current_subset = Subset(current_dataset, subset_indices)\n",
    "#         ret = train_one_epoch(current_subset, model, optimizer, loss_fn, device, epoch, cutmix=True)\n",
    "        \n",
    "#     else:\n",
    "#         ret = train_one_epoch(trn_loader, model, optimizer, loss_fn, device, epoch)\n",
    "\n",
    "\n",
    "#         # 결과 로깅\n",
    "#         log = f\"Epoch: {epoch}\\n\"\n",
    "#         for k, v in ret.items():\n",
    "#             log += f\"{k}: {v:.4f}\\n\"\n",
    "#         print(log)\n",
    "\n",
    "#         scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 2...\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import torch\n",
    "\n",
    "# # Define the path to your saved model checkpoint\n",
    "# folder_path = \"/root/CV_PJT/CV_PJT/model\"\n",
    "# model_save_path = os.path.join(folder_path, \"entire_model_resnext_(1).pth\")\n",
    "\n",
    "# # Check if the checkpoint file exists\n",
    "# if os.path.exists(model_save_path):\n",
    "#     # Load the saved model checkpoint\n",
    "#     checkpoint = torch.load(model_save_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "#     # Load the model state\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "#     # Load the optimizer state\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "#     # Load the starting epoch for resuming\n",
    "#     start_epoch = checkpoint['epoch'] + 1  # Start from the next epoch\n",
    "#     print(f\"Resuming training from epoch {start_epoch}...\")\n",
    "# else:\n",
    "#     print(\"No checkpoint found. Starting training from scratch.\")\n",
    "#     start_epoch = 1  # Start from the first epoch if no checkpoint exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.2774: 100%|██████████| 9226/9226 [2:29:29<00:00,  1.03it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "train_loss: 1.0458\n",
      "train_acc: 0.5371\n",
      "train_f1: 0.5355\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0161: 100%|██████████| 9226/9226 [2:30:26<00:00,  1.02it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "train_loss: 0.0761\n",
      "train_acc: 0.9737\n",
      "train_f1: 0.9737\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1401: 100%|██████████| 9226/9226 [2:27:53<00:00,  1.04it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "train_loss: 0.0537\n",
      "train_acc: 0.9815\n",
      "train_f1: 0.9815\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0537: 100%|██████████| 9226/9226 [2:30:21<00:00,  1.02it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "train_loss: 0.0416\n",
      "train_acc: 0.9857\n",
      "train_f1: 0.9857\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0030: 100%|██████████| 9226/9226 [2:29:42<00:00,  1.03it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "train_loss: 0.0234\n",
      "train_acc: 0.9920\n",
      "train_f1: 0.9920\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0132:   7%|▋         | 619/9226 [10:17<2:23:01,  1.00it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m     ret \u001b[38;5;241m=\u001b[39m train_one_epoch(trn_loader, model, optimizer, loss_fn, device, epoch, cutmix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrn_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Log the results\u001b[39;00m\n\u001b[1;32m     19\u001b[0m log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[89], line 41\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(loader, model, optimizer, loss_fn, device, epoch, mixup, cutmix)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Backpropagation and optimization step\u001b[39;00m\n\u001b[1;32m     40\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 41\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Update metrics\u001b[39;00m\n\u001b[1;32m     44\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:137\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    136\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    214\u001b[0m         group,\n\u001b[1;32m    215\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         state_steps,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:592\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    590\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_addcdiv_(device_params, device_exp_avgs, exp_avg_sq_sqrt)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 592\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    593\u001b[0m         \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[1;32m    594\u001b[0m     ]\n\u001b[1;32m    595\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    596\u001b[0m         \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[1;32m    597\u001b[0m     ]\n\u001b[1;32m    599\u001b[0m     step_size \u001b[38;5;241m=\u001b[39m _stack_if_compiling([(lr \u001b[38;5;241m/\u001b[39m bc) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bias_correction1])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adam.py:593\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    590\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_addcdiv_(device_params, device_exp_avgs, exp_avg_sq_sqrt)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    592\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 593\u001b[0m         \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[1;32m    594\u001b[0m     ]\n\u001b[1;32m    595\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    596\u001b[0m         \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[1;32m    597\u001b[0m     ]\n\u001b[1;32m    599\u001b[0m     step_size \u001b[38;5;241m=\u001b[39m _stack_if_compiling([(lr \u001b[38;5;241m/\u001b[39m bc) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bias_correction1])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:106\u001b[0m, in \u001b[0;36m_get_value\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the total number of epochs you want to run\n",
    "EPOCHS = 100  # Adjust this as necessary for your training\n",
    "\n",
    "# Continue your training loop from start_epoch to EPOCHS\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    # Change the training transform if needed\n",
    "    if epoch == 1:\n",
    "        trn_dataset.transform = get_transform(with_augraphy=False)\n",
    "\n",
    "    # Determine if MixUp or CutMix should be applied\n",
    "    if epoch == 1:\n",
    "        ret = train_one_epoch(trn_loader, model, optimizer, loss_fn, device, epoch, mixup=MixUp)\n",
    "    elif epoch == 2:\n",
    "        ret = train_one_epoch(trn_loader, model, optimizer, loss_fn, device, epoch, cutmix=True)\n",
    "    else:\n",
    "        ret = train_one_epoch(trn_loader, model, optimizer, loss_fn, device, epoch)\n",
    "\n",
    "    # Log the results\n",
    "    log = f\"Epoch: {epoch}\\n\"\n",
    "    for k, v in ret.items():\n",
    "        log += f\"{k}: {v:.4f}\\n\"\n",
    "    print(log)\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Save the checkpoint at the end of the epoch\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the path where you saved the model\n",
    "# Define folder and filename\n",
    "folder_path = \"/root/CV_PJT/CV_PJT/model\"\n",
    "os.makedirs(folder_path, exist_ok=True)  # Create folder if it doesn't exist\n",
    "model_save_path  = os.path.join(folder_path, \"entire_model_resnext.pth\")\n",
    "\n",
    "# At the end of your training loop\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    # Add any other information you want to save, like loss or metrics\n",
    "}, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 8...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Define the path to your saved model checkpoint\n",
    "folder_path = \"/root/CV_PJT/CV_PJT/model\"\n",
    "model_save_path = os.path.join(folder_path, \"entire_model_resnext_(2).pth\")\n",
    "\n",
    "# Check if the checkpoint file exists\n",
    "if os.path.exists(model_save_path):\n",
    "    # Load the saved model checkpoint\n",
    "    checkpoint = torch.load(model_save_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "    # Load the model state\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # Load the optimizer state\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    # Load the starting epoch for resuming\n",
    "    start_epoch = checkpoint['epoch'] + 1  # Start from the next epoch\n",
    "    print(f\"Resuming training from epoch {start_epoch}...\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting training from scratch.\")\n",
    "    start_epoch = 1  # Start from the first epoch if no checkpoint exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0003: 100%|██████████| 4613/4613 [1:16:32<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n",
      "train_loss: 0.0182\n",
      "train_acc: 0.9939\n",
      "train_f1: 0.9939\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0006:   0%|          | 4/4613 [00:04<1:33:44,  1.22s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m     ret \u001b[38;5;241m=\u001b[39m train_one_epoch(current_subset, model, optimizer, loss_fn, device, epoch, cutmix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 47\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrn_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# 결과 로깅\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     log \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[89], line 11\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(loader, model, optimizer, loss_fn, device, epoch, mixup, cutmix)\u001b[0m\n\u001b[1;32m      8\u001b[0m targets_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(loader)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, targets \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m     12\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:418\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices: List[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[_T_co]:\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# add batched sampling support when parent dataset supports it.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# see torch.utils.data._utils.fetch._MapDatasetFetcher\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m--> 418\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[68], line 15\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     13\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, name)))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 15\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img, target\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/albumentations/core/composition.py:210\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    207\u001b[0m     p\u001b[38;5;241m.\u001b[39mpreprocess(data)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(transforms):\n\u001b[0;32m--> 210\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_each_transform:\n\u001b[1;32m    213\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_data_post_transform(data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/albumentations/core/transforms_interface.py:118\u001b[0m, in \u001b[0;36mBasicTransform.__call__\u001b[0;34m(self, force_apply, *args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m             warn(\n\u001b[1;32m    114\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_class_fullname() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m could work incorrectly in ReplayMode for other input data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m because its\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m params depend on targets.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m             )\n\u001b[1;32m    117\u001b[0m         kwargs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_key][\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)] \u001b[38;5;241m=\u001b[39m deepcopy(params)\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_with_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m kwargs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/albumentations/core/transforms_interface.py:131\u001b[0m, in \u001b[0;36mBasicTransform.apply_with_params\u001b[0;34m(self, params, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m     target_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_target_function(key)\n\u001b[1;32m    130\u001b[0m     target_dependencies \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_dependence\u001b[38;5;241m.\u001b[39mget(key, [])}\n\u001b[0;32m--> 131\u001b[0m     res[key] \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtarget_dependencies\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     res[key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[77], line 23\u001b[0m, in \u001b[0;36mAugraphyTransform.apply\u001b[0;34m(self, img, **params)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     22\u001b[0m     img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(img)\n\u001b[0;32m---> 23\u001b[0m augmented_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m augmented_image\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/augraphy/base/augmentationsequence.py:38\u001b[0m, in \u001b[0;36mAugmentationSequence.__call__\u001b[0;34m(self, image, layer, mask, keypoints, bounding_boxes, force)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m     37\u001b[0m     result \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 38\u001b[0m current_result \u001b[38;5;241m=\u001b[39m \u001b[43maugmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeypoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeypoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounding_boxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounding_boxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(augmentation, Augmentation):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (keypoints \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (bounding_boxes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/augraphy/augmentations/scribbles.py:251\u001b[0m, in \u001b[0;36mScribbles.__call__\u001b[0;34m(self, image, layer, mask, keypoints, bounding_boxes, force)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# create an ink generator and generate scribbles\u001b[39;00m\n\u001b[1;32m    228\u001b[0m ink_generator \u001b[38;5;241m=\u001b[39m InkGenerator(\n\u001b[1;32m    229\u001b[0m     ink_type\u001b[38;5;241m=\u001b[39mscribbles_ink,\n\u001b[1;32m    230\u001b[0m     ink_draw_method\u001b[38;5;241m=\u001b[39mscribbles_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    248\u001b[0m     ink_lines_stroke_count_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscribbles_lines_stroke_count_range,\n\u001b[1;32m    249\u001b[0m )\n\u001b[0;32m--> 251\u001b[0m image_output \u001b[38;5;241m=\u001b[39m \u001b[43mink_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_ink\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_alpha:\n\u001b[1;32m    254\u001b[0m     image_output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdstack((image_output, image_alpha))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/augraphy/utilities/inkgenerator.py:855\u001b[0m, in \u001b[0;36mInkGenerator.generate_ink\u001b[0;34m(self, ink_type, ink_draw_method, ink_draw_iterations, ink_location, ink_background, ink_background_size, ink_background_color, ink_color, ink_min_brightness, ink_min_brightness_value_range, ink_draw_size_range, ink_thickness_range, ink_brightness_change, ink_skeletonize, ink_text, ink_text_font, ink_text_rotate_range, ink_lines_coordinates, ink_lines_curvy, ink_lines_stroke_count_range)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;66;03m# generate ink effect\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mink_draw_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlines\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 855\u001b[0m     image_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mink_background\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    857\u001b[0m     image_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_text(ink_background)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/augraphy/utilities/inkgenerator.py:525\u001b[0m, in \u001b[0;36mInkGenerator.generate_lines\u001b[0;34m(self, ink_background)\u001b[0m\n\u001b[1;32m    516\u001b[0m cv2\u001b[38;5;241m.\u001b[39mpolylines(\n\u001b[1;32m    517\u001b[0m     line_image,\n\u001b[1;32m    518\u001b[0m     ink_lines_coordinates,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m     thickness\u001b[38;5;241m=\u001b[39mink_thickness,\n\u001b[1;32m    522\u001b[0m )\n\u001b[1;32m    524\u001b[0m \u001b[38;5;66;03m# apply line image with ink effect to background\u001b[39;00m\n\u001b[0;32m--> 525\u001b[0m line_background \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_ink_effect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline_background\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;66;03m# reassign background patch to background\u001b[39;00m\n\u001b[1;32m    528\u001b[0m lines_background[ystart : ystart \u001b[38;5;241m+\u001b[39m ysize, xstart : xstart \u001b[38;5;241m+\u001b[39m xsize] \u001b[38;5;241m=\u001b[39m line_background\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/augraphy/utilities/inkgenerator.py:585\u001b[0m, in \u001b[0;36mInkGenerator.apply_ink_effect\u001b[0;34m(self, foreground_image, background_image)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;66;03m# marker\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mink_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmarker\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 585\u001b[0m     image_merged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_marker_effect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforeground_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;66;03m# highlighter\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    588\u001b[0m     image_merged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_highlighter_effect(foreground_image, background_image)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/augraphy/utilities/inkgenerator.py:232\u001b[0m, in \u001b[0;36mInkGenerator.apply_marker_effect\u001b[0;34m(self, ink_image, ink_background)\u001b[0m\n\u001b[1;32m    229\u001b[0m ink_image[ink_image_gray \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m255\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mink_color\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# Last step, blur image for better effect\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m ink_image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGaussianBlur\u001b[49m\u001b[43m(\u001b[49m\u001b[43mink_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mmultiply(ink_image, ink_background, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Continue your training loop from start_epoch to EPOCHS\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    if epoch == 0:\n",
    "        trn_dataset.transform = get_transform(with_augraphy=True)\n",
    "        \n",
    "    # 데이터셋 epoch 업데이트\n",
    "        trn_dataset.epoch = epoch\n",
    "        \n",
    "        # 세 번째 에포크부터는 데이터의 30%만 사용\n",
    "    if epoch >= 3:\n",
    "        indices = list(range(len(trn_dataset)))\n",
    "        random.shuffle(indices)\n",
    "        subset_size = int(0.5 * len(trn_dataset))\n",
    "        subset_indices = indices[:subset_size]\n",
    "        current_dataset = Subset(trn_dataset, subset_indices)\n",
    "    else:\n",
    "        current_dataset = trn_dataset\n",
    "            \n",
    "    trn_loader = DataLoader(\n",
    "        current_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Determine if MixUp or CutMix should be applied\n",
    "    if epoch == 1:\n",
    "        # Create a subset of 30% for MixUp\n",
    "        indices = list(range(len(current_dataset)))\n",
    "        random.shuffle(indices)\n",
    "        subset_size = int(0.3 * len(current_dataset))  # 30% of current dataset\n",
    "        subset_indices = indices[:subset_size]\n",
    "        current_subset = Subset(current_dataset, subset_indices)\n",
    "        ret = train_one_epoch(current_subset, model, optimizer, loss_fn, device, epoch, mixup=MixUp)\n",
    "                              \n",
    "    elif epoch == 2:\n",
    "        # Create a subset of 30% for CutMix\n",
    "        indices = list(range(len(current_dataset)))\n",
    "        random.shuffle(indices)\n",
    "        subset_size = int(0.3 * len(current_dataset))  # 30% of current dataset\n",
    "        subset_indices = indices[:subset_size]\n",
    "        current_subset = Subset(current_dataset, subset_indices)\n",
    "        ret = train_one_epoch(current_subset, model, optimizer, loss_fn, device, epoch, cutmix=True)\n",
    "        \n",
    "    else:\n",
    "        ret = train_one_epoch(trn_loader, model, optimizer, loss_fn, device, epoch)\n",
    "\n",
    "\n",
    "        # 결과 로깅\n",
    "        log = f\"Epoch: {epoch}\\n\"\n",
    "        for k, v in ret.items():\n",
    "            log += f\"{k}: {v:.4f}\\n\"\n",
    "        print(log)\n",
    "\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the path where you saved the model\n",
    "# Define folder and filename\n",
    "folder_path = \"/root/CV_PJT/CV_PJT/model\"\n",
    "os.makedirs(folder_path, exist_ok=True)  # Create folder if it doesn't exist\n",
    "model_save_path  = os.path.join(folder_path, \"entire_model_resnext_(3).pth\")\n",
    "\n",
    "# At the end of your training loop\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    # Add any other information you want to save, like loss or metrics\n",
    "}, model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkwxRXoBpbaX"
   },
   "source": [
    "# 6. Inference & Save File\n",
    "* 테스트 이미지에 대한 추론을 진행하고, 결과 파일을 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZKpXm2MnE43",
    "outputId": "1ff8a39c-3285-4ff7-eb79-90658aeb8dfb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:16<00:00,  6.08it/s]\n"
     ]
    }
   ],
   "source": [
    "preds_list = []\n",
    "\n",
    "model.eval()\n",
    "for image, _ in tqdm(tst_loader):\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(image)\n",
    "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aClN7Qi7VZoh"
   },
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(tst_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDBXQqAzVvLY"
   },
   "outputs": [],
   "source": [
    "sample_submission_df = pd.read_csv(\"/root/CV_PJT/CV_PJT/data/data/sample_submission.csv\")\n",
    "assert (sample_submission_df['ID'] == pred_df['ID']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePx2vCELVnuS"
   },
   "outputs": [],
   "source": [
    "pred_df.to_csv(\"pred_22.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9yMO8s6GqAwZ",
    "outputId": "9a30616f-f0ea-439f-a906-dd806737ce00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3140"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fO-LZvECnE47"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDCsJuKunE47",
    "outputId": "2fbd79dd-e0fb-4c4c-d8d3-e80a9cbc3706"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_273216/3309000382.py:5: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.countplot(x='target', data=pred_df, palette='Set2')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Count the number of predictions per class\n",
    "sns.countplot(x='target', data=pred_df, palette='Set2')\n",
    "plt.title('Distribution of Predicted Classes')\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "12ed2251de294e75b1b58ca4eff7895a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_287810d9b1fa4765a3833cada619a6c4",
       "IPY_MODEL_ec9e1134340045b3bc301f540fb6e076",
       "IPY_MODEL_fc8365dc38004f10b2da20f982550531"
      ],
      "layout": "IPY_MODEL_a53f80c0db9e4bd89c210e80cf147fb0"
     }
    },
    "1ea4b022f29840078e224d33e4a60448": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "287810d9b1fa4765a3833cada619a6c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c29ede4e42784e2cb6ec9f2897b84d00",
      "placeholder": "​",
      "style": "IPY_MODEL_309aad3b1b2e42538efd3c43ef2ee7ce",
      "value": "model.safetensors: 100%"
     }
    },
    "309aad3b1b2e42538efd3c43ef2ee7ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3f685cc920cf4d8f8cdcfea9d7d006ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68ead17e3dba4e4483721871e20e59ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "892cf8d9abfc439f89a334e5e742518f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a53f80c0db9e4bd89c210e80cf147fb0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c29ede4e42784e2cb6ec9f2897b84d00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec9e1134340045b3bc301f540fb6e076": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f685cc920cf4d8f8cdcfea9d7d006ed",
      "max": 49335454,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_892cf8d9abfc439f89a334e5e742518f",
      "value": 49335454
     }
    },
    "fc8365dc38004f10b2da20f982550531": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68ead17e3dba4e4483721871e20e59ff",
      "placeholder": "​",
      "style": "IPY_MODEL_1ea4b022f29840078e224d33e4a60448",
      "value": " 49.3M/49.3M [00:00&lt;00:00, 98.8MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
