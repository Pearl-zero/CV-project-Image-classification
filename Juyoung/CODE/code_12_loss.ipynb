{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 9396,
     "status": "ok",
     "timestamp": 1700314592802,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "3BaoIkv5Xwa0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드를 고정합니다.\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1700314772722,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "Hyl8oAy6TZAu"
   },
   "outputs": [],
   "source": [
    "# 데이터셋 클래스를 정의합니다.\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1700315066028,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "kTECBJfVTbdl"
   },
   "outputs": [],
   "source": [
    "# one epoch 학습을 위한 함수입니다.\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_f1\": train_f1,\n",
    "    }\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_loss(loss_fn, pred, labels_a, labels_b, lam):\n",
    "    return lam * loss_fn(pred, labels_a) + (1 - lam) * loss_fn(pred, labels_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Define Focal Loss Function\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce_loss(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Define Data Augmentation (for specific classes)\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "default_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/root/CV_PJT/CV_PJT/code/train2.csv')\n",
    "sample_submission_df = pd.read_csv(\"/root/CV_PJT/CV_PJT/data/data/sample_submission.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Wjom43UvoXcx"
   },
   "source": [
    "## 3. Hyper-parameters\n",
    "* 학습 및 추론에 필요한 하이퍼파라미터들을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1700315112439,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "KByfAeRmXwYk"
   },
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# data config\n",
    "data_path = 'datasets_fin/'\n",
    "\n",
    "# model config\n",
    "model_name = 'resnet34'\n",
    "\n",
    "# training config\n",
    "img_size = 260\n",
    "LR = 1e-4\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "num_workers = 0\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focal Loss hyperparameters (if using)\n",
    "ALPHA = 1\n",
    "GAMMA = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "amum-FlIojc6"
   },
   "source": [
    "## 4. Load Data\n",
    "* 학습, 테스트 데이터셋과 로더를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1700315112439,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "llh5C7ZKoq2S"
   },
   "outputs": [],
   "source": [
    "# augmentation을 위한 transform 코드\n",
    "trn_transform = A.Compose([\n",
    "    # 이미지 크기 조정\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    # images normalization\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    # numpy 이미지나 PIL 이미지를 PyTorch 텐서로 변환\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# test image 변환을 위한 transform 코드\n",
    "tst_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 118387\n",
      "Validation dataset size: 29597\n",
      "Test dataset size: 3140\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Dataset definition\n",
    "full_dataset = ImageDataset(\n",
    "    \"/root/CV_PJT/CV_PJT/code/train2.csv\",\n",
    "    \"/root/CV_PJT/CV_PJT/data/data/train\",\n",
    "    transform=trn_transform\n",
    ")\n",
    "\n",
    "# Calculate the total number of samples in the dataset\n",
    "dataset_size = len(full_dataset)\n",
    "\n",
    "# Define the ratios for training and validation\n",
    "train_ratio = 0.8  # Use 80% of the data for training\n",
    "val_ratio = 1 - train_ratio  # Remaining 20% for validation\n",
    "\n",
    "# Calculate the number of samples for training and validation\n",
    "train_size = int(train_ratio * dataset_size)\n",
    "val_size = dataset_size - train_size  # Ensure all samples are accounted for\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "trn_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Calculate class weights based on training data labels (for CrossEntropyLoss with class weights)\n",
    "train_labels = [label for _, label in trn_dataset]\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Use WeightedRandomSampler for oversampling in training set (optional)\n",
    "class_sample_count = np.array([len(np.where(train_labels == t)[0]) for t in np.unique(train_labels)])\n",
    "weights = 1. / class_sample_count\n",
    "samples_weights = np.array([weights[t] for t in train_labels])\n",
    "sampler = WeightedRandomSampler(samples_weights, num_samples=len(samples_weights), replacement=True)\n",
    "\n",
    "# DataLoader with sampler for oversampling (if needed)\n",
    "train_loader = DataLoader(trn_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Define the test dataset\n",
    "tst_dataset = ImageDataset(\n",
    "    \"/root/CV_PJT/CV_PJT/data/data/sample_submission.csv\",\n",
    "    \"/root/CV_PJT/CV_PJT/data/data/test\",\n",
    "    transform=tst_transform\n",
    ")\n",
    "\n",
    "# Print the sizes of the datasets\n",
    "print(\"Training dataset size:\", len(trn_dataset))\n",
    "print(\"Validation dataset size:\", len(val_dataset))\n",
    "print(\"Test dataset size:\", len(tst_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1700315112808,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "_sO03fWaQj1h"
   },
   "outputs": [],
   "source": [
    "# DataLoader 정의\n",
    "trn_loader = DataLoader(\n",
    "    trn_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "tst_loader = DataLoader(\n",
    "    tst_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Model\n",
    "* 모델을 로드하고, 학습을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "    \n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "    \n",
    "    ret = {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"val_f1\": val_f1,\n",
    "    }\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0099: 100%|██████████| 2467/2467 [04:55<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.018164). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0492: 100%|██████████| 2467/2467 [04:53<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.018164 --> 0.004674). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0032: 100%|██████████| 2467/2467 [04:55<00:00,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.004674 --> 0.002492). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0111: 100%|██████████| 2467/2467 [04:55<00:00,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.002492 --> 0.001375). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0003: 100%|██████████| 2467/2467 [04:55<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0002: 100%|██████████| 2467/2467 [04:54<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0002: 100%|██████████| 2467/2467 [04:55<00:00,  8.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0005: 100%|██████████| 2467/2467 [04:55<00:00,  8.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000: 100%|██████████| 2467/2467 [04:55<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "Fold 0 completed. Best val accuracy: 0.9841877201429187, Best val F1 Score: 0.9843874880684623\n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2275: 100%|██████████| 2467/2467 [04:56<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 0.014381). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0003: 100%|██████████| 2467/2467 [04:54<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.014381 --> 0.004218). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0010: 100%|██████████| 2467/2467 [04:55<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.004218 --> 0.002152). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0012: 100%|██████████| 2467/2467 [04:55<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0009: 100%|██████████| 2467/2467 [04:55<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000: 100%|██████████| 2467/2467 [04:55<00:00,  8.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (0.002152 --> 0.000810). Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0229: 100%|██████████| 2467/2467 [04:57<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0078: 100%|██████████| 2467/2467 [04:57<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000: 100%|██████████| 2467/2467 [04:56<00:00,  8.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0002: 100%|██████████| 2467/2467 [04:56<00:00,  8.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000: 100%|██████████| 2467/2467 [04:57<00:00,  8.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m fold_performance[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_f1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_f1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Validation Phase\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m val_results \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m fold_performance[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(val_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     49\u001b[0m fold_performance[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_f1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(val_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_f1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(loader, model, loss_fn, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m targets_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m image, targets \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m      9\u001b[0m         image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m         targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Dataset & K-Fold Configuration\n",
    "dataset = trn_dataset\n",
    "k_folds = 3\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Placeholder for Results\n",
    "fold_results = []\n",
    "\n",
    "# K-Fold Cross-Validation Process\n",
    "for fold, (train_ids, valid_ids) in enumerate(kfold.split(dataset)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    # Data Loaders for Train and Validation\n",
    "    train_subsampler = Subset(dataset, train_ids)\n",
    "    valid_subsampler = Subset(dataset, valid_ids)\n",
    "    train_loader = DataLoader(train_subsampler, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)\n",
    "    valid_loader = DataLoader(valid_subsampler, batch_size=BATCH_SIZE, shuffle=False, num_workers=1)\n",
    "\n",
    "    # Model, Loss, Optimizer Initialization\n",
    "    model = timm.create_model('resnet34', pretrained=True, num_classes=17).to(device)\n",
    "    # loss_fn = nn.CrossEntropyLoss()\n",
    "    # Option 2: Use Focal Loss instead of CrossEntropyLoss (if needed)\n",
    "    loss_fn = FocalLoss(alpha=ALPHA, gamma=GAMMA)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    # Initialize Early Stopping\n",
    "    early_stopping = EarlyStopping(patience=5, verbose=True, delta=0.001,\n",
    "                                   path=f'/root/CV_PJT/CV_PJT/code/fold3/resnet34_fold_{fold}_checkpoint.pt')\n",
    "\n",
    "    # Performance Tracking for Each Fold\n",
    "    fold_performance = {\n",
    "        'train_acc': [],\n",
    "        'train_f1': [],\n",
    "        'val_acc': [],\n",
    "        'val_f1': []\n",
    "    }\n",
    "\n",
    "    # Training Loop with Early Stopping\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Training Phase\n",
    "        train_results = train_one_epoch(train_loader, model, optimizer, loss_fn, device)\n",
    "        fold_performance['train_acc'].append(train_results['train_acc'])\n",
    "        fold_performance['train_f1'].append(train_results['train_f1'])\n",
    "\n",
    "        # Validation Phase\n",
    "        val_results = validate(valid_loader, model, loss_fn, device)\n",
    "        fold_performance['val_acc'].append(val_results['val_acc'])\n",
    "        fold_performance['val_f1'].append(val_results['val_f1'])\n",
    "\n",
    "        # Early Stopping Check\n",
    "        early_stopping(val_results['val_loss'], model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # Load Best Model for This Fold\n",
    "    model.load_state_dict(torch.load(f'/root/CV_PJT/CV_PJT/code/fold3/resnet34_fold_{fold}_checkpoint.pt'))\n",
    "\n",
    "    # Store Results for This Fold\n",
    "    fold_results.append(fold_performance)\n",
    "    print(f'Fold {fold} completed. Best val accuracy: {val_results[\"val_acc\"]}, Best val F1 Score: {val_results[\"val_f1\"]}\\n')\n",
    "\n",
    "# Average Performance Across Folds\n",
    "train_accs = [np.mean(fold['train_acc']) for fold in fold_results]\n",
    "train_f1s = [np.mean(fold['train_f1']) for fold in fold_results]\n",
    "val_accs = [np.mean(fold['val_acc']) for fold in fold_results]\n",
    "val_f1s = [np.mean(fold['val_f1']) for fold in fold_results]\n",
    "\n",
    "print(f'Average Train Accuracy: {np.mean(train_accs)}, Average Train F1 Score: {np.mean(train_f1s)}')\n",
    "print(f'Average Validation Accuracy: {np.mean(val_accs)}, Average Validation F1 Score: {np.mean(val_f1s)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 870,
     "status": "ok",
     "timestamp": 1700315114067,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "FbBgFPsLT-CO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# load model\\nmodel = timm.create_model(\\n    model_name,\\n    pretrained=True,\\n    num_classes=17\\n).to(device)\\nloss_fn = nn.CrossEntropyLoss()\\noptimizer = Adam(model.parameters(), lr=LR)'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "model = timm.create_model(\n",
    "    model_name,\n",
    "    pretrained=True,\n",
    "    num_classes=17\n",
    ").to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8778,
     "status": "ok",
     "timestamp": 1700315122843,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "OvIVcSRgUPtS",
    "outputId": "88230bf2-976f-45f6-b3b7-1a2d0ad00548"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for epoch in range(EPOCHS):\\n    ret = train_one_epoch(trn_loader, model, optimizer, loss_fn, device=device)\\n    ret[\\'epoch\\'] = epoch\\n\\n    log = \"\"\\n    for k, v in ret.items():\\n      log += f\"{k}: {v:.4f}\\n\"\\n    print(log)'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    ret = train_one_epoch(trn_loader, model, optimizer, loss_fn, device=device)\n",
    "    ret['epoch'] = epoch\n",
    "\n",
    "    log = \"\"\n",
    "    for k, v in ret.items():\n",
    "      log += f\"{k}: {v:.4f}\\n\"\n",
    "    print(log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lkwxRXoBpbaX"
   },
   "source": [
    "# 6. Inference & Save File\n",
    "* 테스트 이미지에 대한 추론을 진행하고, 결과 파일을 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12776,
     "status": "ok",
     "timestamp": 1700315185336,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "uRYe6jlPU_Om",
    "outputId": "2a08690c-9ffe-418d-8679-eb9280147110"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732c4a0cf5ed4a9dbf122f68a39e3de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting with fold 0:   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665bf3ea906e43fe87938fc1c1626ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting with fold 1:   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3140"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 테스트 데이터셋에 대한 각 폴드의 모델 예측 수행\n",
    "k_folds = 2\n",
    "\n",
    "predictions = []\n",
    "for fold in range(k_folds):\n",
    "    # 각 폴드의 모델을 로드합니다.\n",
    "    model.load_state_dict(torch.load(f'/root/CV_PJT/CV_PJT/code/fold3/resnet34_fold_{fold}_checkpoint.pt'))\n",
    "    model.eval()\n",
    "    fold_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(tst_loader, desc=f'Predicting with fold {fold}'):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            fold_predictions.extend(predicted.cpu().numpy())\n",
    "    predictions.append(fold_predictions)\n",
    "\n",
    "# 앙상블 예측: 다수결 보팅\n",
    "predictions = np.array(predictions)\n",
    "predictions = np.transpose(predictions, (1, 0))\n",
    "\n",
    "preds_list = []\n",
    "for i in range(len(predictions)):\n",
    "    mode_result = mode(predictions[i])\n",
    "    preds_list.append(mode_result[0])\n",
    "    \n",
    "len(preds_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1700315216829,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "aClN7Qi7VZoh"
   },
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(tst_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1700315238836,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "VDBXQqAzVvLY"
   },
   "outputs": [],
   "source": [
    "sample_submission_df = pd.read_csv(\"/root/CV_PJT/CV_PJT/data/data/sample_submission.csv\")\n",
    "assert (sample_submission_df['ID'] == pred_df['ID']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1700315244710,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "ePx2vCELVnuS"
   },
   "outputs": [],
   "source": [
    "pred_df.to_csv(\"pred_12.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_192127/3309000382.py:5: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.countplot(x='target', data=pred_df, palette='Set2')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Count the number of predictions per class\n",
    "sns.countplot(x='target', data=pred_df, palette='Set2')\n",
    "plt.title('Distribution of Predicted Classes')\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.read_csv('/root/CV_PJT/CV_PJT/code/pred_12.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Misclassified Images\n",
    "num_images = min(len(misclassified_images), 16)  # Display up to 16 images\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(4, 4, i + 1)  # 4x4 grid for images\n",
    "    plt.imshow(misclassified_images[i].permute(1, 2, 0))  # Change channel order for plotting\n",
    "    plt.title(f'True: {misclassified_labels[i]}, Predicted: {misclassified_preds[i]}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Now, assuming you have a list of test image filenames:\n",
    "test_image_filenames = [...]  # Load or define your test image filenames here\n",
    "\n",
    "# Displaying the actual test images corresponding to the misclassified images\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(4, 4, i + 1)  # 4x4 grid for images\n",
    "    test_image_index = misclassified_indices[i]  # Index of the misclassified image in the valid set\n",
    "    plt.imshow(misclassified_images[i].permute(1, 2, 0))  # Display the misclassified image\n",
    "    plt.title(f'File: {test_image_filenames[test_image_index]}')  # Show corresponding test filename\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
